{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40aa268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check python version\n",
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- --------------------\n",
      "alabaster                     0.7.12\n",
      "anaconda-client               1.11.0\n",
      "anaconda-navigator            2.4.0\n",
      "anaconda-project              0.11.1\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.2\n",
      "astroid                       2.11.7\n",
      "astropy                       5.1\n",
      "async-generator               1.10\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         21.4.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.9.1\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.11.1\n",
      "binaryornot                   0.4.4\n",
      "bitarray                      2.5.1\n",
      "bkcharts                      0.2\n",
      "black                         22.6.0\n",
      "bleach                        4.1.0\n",
      "bokeh                         2.4.3\n",
      "boto3                         1.24.28\n",
      "botocore                      1.27.28\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "cachetools                    5.3.0\n",
      "certifi                       2022.9.14\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.0.4\n",
      "cloudpickle                   2.0.0\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.5\n",
      "colorcet                      3.0.0\n",
      "comtypes                      1.1.10\n",
      "conda                         22.9.0\n",
      "conda-build                   3.22.0\n",
      "conda-content-trust           0.1.3\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        1.9.0\n",
      "conda-repo-cli                1.0.20\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "constantly                    15.1.0\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  37.0.1\n",
      "cssselect                     1.1.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.32\n",
      "cytoolz                       0.11.0\n",
      "daal4py                       2021.6.0\n",
      "dask                          2022.7.0\n",
      "datashader                    0.14.1\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.5.1\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.4\n",
      "distributed                   2022.7.0\n",
      "docutils                      0.18.1\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "exceptiongroup                1.1.1\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.6.0\n",
      "flake8                        4.0.1\n",
      "Flask                         1.1.2\n",
      "fonttools                     4.25.0\n",
      "fsspec                        2022.7.1\n",
      "future                        0.18.2\n",
      "gensim                        4.1.2\n",
      "glob2                         0.7\n",
      "google-auth                   2.18.1\n",
      "google-auth-oauthlib          1.0.0\n",
      "greenlet                      1.1.1\n",
      "gspread                       5.9.0\n",
      "h11                           0.14.0\n",
      "h5py                          3.7.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.15.0\n",
      "httplib2                      0.22.0\n",
      "hvplot                        0.8.0\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.3\n",
      "imagecodecs                   2021.8.26\n",
      "imageio                       2.19.3\n",
      "imagesize                     1.4.1\n",
      "importlib-metadata            4.11.3\n",
      "incremental                   21.3.0\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "intake                        0.6.5\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.15.2\n",
      "ipython                       7.31.1\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.5\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  2.0.1\n",
      "jdcal                         1.4.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     0.9.0\n",
      "Jinja2                        2.11.3\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.1.0\n",
      "json5                         0.9.6\n",
      "jsonschema                    4.16.0\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.3.4\n",
      "jupyter-console               6.4.3\n",
      "jupyter_core                  4.11.1\n",
      "jupyter-server                1.18.1\n",
      "jupyterlab                    3.4.4\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-server             2.10.3\n",
      "jupyterlab-widgets            1.0.0\n",
      "jupyterthemes                 0.20.0\n",
      "keyring                       23.4.0\n",
      "kiwisolver                    1.4.2\n",
      "lazy-object-proxy             1.6.0\n",
      "lesscpy                       0.15.1\n",
      "libarchive-c                  2.9\n",
      "llvmlite                      0.38.0\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.1\n",
      "lz4                           3.1.3\n",
      "Markdown                      3.3.4\n",
      "MarkupSafe                    2.0.1\n",
      "matplotlib                    3.5.2\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.6.1\n",
      "menuinst                      1.4.19\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.1\n",
      "mkl-random                    1.2.2\n",
      "mkl-service                   2.4.0\n",
      "mock                          4.0.3\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.3\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "mypy-extensions               0.4.3\n",
      "navigator-updater             0.3.0\n",
      "nbclassic                     0.3.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.4.4\n",
      "nbformat                      5.5.0\n",
      "nest-asyncio                  1.5.5\n",
      "networkx                      2.8.4\n",
      "nltk                          3.7\n",
      "nose                          1.3.7\n",
      "notebook                      6.4.12\n",
      "numba                         0.55.1\n",
      "numexpr                       2.8.3\n",
      "numpy                         1.21.5\n",
      "numpydoc                      1.4.0\n",
      "oauth2client                  4.1.3\n",
      "oauthlib                      3.2.2\n",
      "olefile                       0.46\n",
      "openpyxl                      3.0.10\n",
      "outcome                       1.2.0\n",
      "packaging                     21.3\n",
      "pandas                        1.4.4\n",
      "pandocfilters                 1.5.0\n",
      "panel                         0.13.1\n",
      "param                         1.12.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.2.0\n",
      "pathlib                       1.0.1\n",
      "pathspec                      0.9.0\n",
      "patsy                         0.5.2\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.2.0\n",
      "pip                           22.2.2\n",
      "pkginfo                       1.8.2\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "ply                           3.11\n",
      "poyo                          0.5.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.20\n",
      "Protego                       0.1.16\n",
      "psutil                        5.9.0\n",
      "ptyprocess                    0.7.0\n",
      "py                            1.11.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.8.0\n",
      "pycosat                       0.6.3\n",
      "pycparser                     2.21\n",
      "pyct                          0.4.8\n",
      "pycurl                        7.45.1\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.1.1\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      2.4.0\n",
      "Pygments                      2.11.2\n",
      "PyHamcrest                    2.0.2\n",
      "PyJWT                         2.4.0\n",
      "pylint                        2.14.5\n",
      "pyls-spyder                   0.4.0\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.34\n",
      "pyOpenSSL                     22.0.0\n",
      "pyparsing                     3.0.9\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.1.2\n",
      "python-dateutil               2.8.2\n",
      "python-lsp-black              1.0.0\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.3.3\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.0\n",
      "pytz                          2022.1\n",
      "pyviz-comms                   2.0.2\n",
      "PyWavelets                    1.3.0\n",
      "pywin32                       302\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.2\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.1.10\n",
      "QtAwesome                     1.0.3\n",
      "qtconsole                     5.2.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.5.0\n",
      "regex                         2022.7.9\n",
      "requests                      2.28.1\n",
      "requests-file                 1.5.1\n",
      "requests-oauthlib             1.3.1\n",
      "rope                          0.22.0\n",
      "rsa                           4.9\n",
      "Rtree                         0.9.7\n",
      "ruamel-yaml-conda             0.15.100\n",
      "s3transfer                    0.6.0\n",
      "scikit-image                  0.19.2\n",
      "scikit-learn                  1.0.2\n",
      "scikit-learn-intelex          2021.20221004.171935\n",
      "scipy                         1.9.1\n",
      "Scrapy                        2.6.2\n",
      "seaborn                       0.11.2\n",
      "selenium                      4.9.1\n",
      "Send2Trash                    1.8.0\n",
      "service-identity              18.1.0\n",
      "setuptools                    63.4.1\n",
      "sip                           4.19.13\n",
      "six                           1.16.0\n",
      "smart-open                    5.2.1\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcollections             2.1.0\n",
      "sortedcontainers              2.4.0\n",
      "soupsieve                     2.3.1\n",
      "Sphinx                        5.0.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder                        5.2.2\n",
      "spyder-kernels                2.2.1\n",
      "SQLAlchemy                    1.4.39\n",
      "statsmodels                   0.13.2\n",
      "sympy                         1.10.1\n",
      "tables                        3.6.1\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.0.1\n",
      "terminado                     0.13.1\n",
      "testpath                      0.6.0\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2021.7.2\n",
      "tinycss                       0.4\n",
      "tldextract                    3.2.0\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.11.2\n",
      "tornado                       6.1\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.1.1\n",
      "trio                          0.22.0\n",
      "trio-websocket                0.10.2\n",
      "Twisted                       22.2.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typing_extensions             4.3.0\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "urllib3                       1.26.11\n",
      "w3lib                         1.21.0\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      2.0.3\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            3.5.2\n",
      "win-inet-pton                 1.1.0\n",
      "win-unicode-console           0.5\n",
      "wincertstore                  0.2\n",
      "wrapt                         1.14.1\n",
      "wsproto                       1.2.0\n",
      "xarray                        0.20.1\n",
      "xlrd                          2.0.1\n",
      "XlsxWriter                    3.0.3\n",
      "xlwings                       0.27.15\n",
      "yapf                          0.31.0\n",
      "zict                          2.1.0\n",
      "zipp                          3.8.0\n",
      "zope.interface                5.4.0\n"
     ]
    }
   ],
   "source": [
    "# check your library list\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd819a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install selenium \n",
    "  # pip install selenium # uncomment and run for install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f5ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install beautifulsoup4\n",
    "# pip install beautifulsoup4 # uncomment and run for install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31da828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shawshank Redemption\n",
      "The Godfather\n",
      "The Dark Knight\n",
      "The Godfather Part II\n",
      "12 Angry Men\n",
      "Schindler's List\n",
      "The Lord of the Rings: The Return of the King\n",
      "Pulp Fiction\n",
      "The Lord of the Rings: The Fellowship of the Ring\n",
      "Il buono, il brutto, il cattivo\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "open https://www.imdb.com/chart/top/ , right click on first title, click inspect or inspect element,\n",
    "By pressing CTRL+F and searching in the HTML code structure, you will see that there is only one <table> \n",
    "tag on the page. This is useful as it gives us information about how we can access the data.\n",
    "\n",
    "An HTML selector that will give us all of the titles from the page is table tbody tr td.titleColumn a. \n",
    "That’s because all titles are in an anchor inside a table cell with the class “titleColumn”.\n",
    "\n",
    "Using this CSS selector and getting the innerText of each anchor will give us the titles that we need. \n",
    "You can simulate that in the browser console from the new window you just opened and by using the \n",
    "JavaScript line: document.querySelectorAll(\"table tbody tr td.titleColumn a\")[0].innerText\n",
    "\n",
    "click on console tab and paste the JS line\n",
    "\n",
    "Now that we have this selector, we can start writing our Python code and extracting the information we need.\n",
    "\n",
    "How to Use BeautifulSoup to Extract Statically Loaded Content\n",
    "The movie titles from our list are static content. That’s because if you look into the \n",
    "page source (CTRL+U on the page or right-click and then choose View Page Source), you will see that \n",
    "the titles are already there.\n",
    "\n",
    "Static content is usually easier to scrape as it doesn’t require JavaScript rendering. \n",
    "To extract the first ten titles on the list, we will use BeautifulSoup to get the content \n",
    "and then print it in the output of our scraper.\n",
    "\n",
    "The code below uses the selector we saw in the first step to extract the movie title anchors from the page.\n",
    "It then loops through the first ten and displays the innerText of each.\n",
    "'''\n",
    "\n",
    "import requests # import the library we need\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "page = requests.get('https://www.imdb.com/chart/top/') # Getting page HTML through request\n",
    "soup = BeautifulSoup(page.content, 'html.parser') # Parsing content using beautifulsoup\n",
    " \n",
    "links = soup.select(\"table tbody tr td.titleColumn a\") # Selecting all of the anchors with titles\n",
    "first10 = links[:10] # Keep only the first 10 anchors\n",
    "for anchor in first10:\n",
    "    print(anchor.text) # Display the innerText of each anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5831371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wibow\\AppData\\Local\\Temp\\ipykernel_21900\\3565873016.py:34: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(r'C:\\Users\\wibow\\OneDrive\\Desktop\\Web Scraping\\chromedriver_win32\\chromedriver.exe', options=option)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shawshank Redemption\n",
      "The Godfather\n",
      "The Dark Knight\n",
      "The Godfather Part II\n",
      "12 Angry Men\n",
      "Schindler's List\n",
      "The Lord of the Rings: The Return of the King\n",
      "Pulp Fiction\n",
      "The Lord of the Rings: The Fellowship of the Ring\n",
      "Il buono, il brutto, il cattivo\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "How to Extract Dynamically Loaded Content\n",
    "As technology advanced, websites started to load their content dynamically. This improves the page’s \n",
    "performance, the user's experience, and even removes an extra barrier for scrapers.\n",
    "\n",
    "This complicates things, though, as the HTML retrieved from a simple request will not contain the dynamic \n",
    "content. Fortunately, with Selenium, we can simulate a request in the browser and wait for the dynamic \n",
    "content to be displayed.\n",
    "\n",
    "How to Use Selenium for Requests\n",
    "You will need to know the location of your chromedriver. The following code is identical to the one \n",
    "presented in the second step, but this time we are using Selenium to make the request. We will still \n",
    "parse the page’s content using BeautifulSoup, as we did before.\n",
    "\n",
    "Don’t forget to replace “YOUR-PATH-TO-CHROMEDRIVER” with the location where you extracted the chromedriver.\n",
    "Also, you should notice that instead of page.content, when we are creating the BeautifulSoup object, we are\n",
    "now using driver.page_source, which provides the HTML content of the page.\n",
    "'''\n",
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    " \n",
    " # define option\n",
    "option = webdriver.ChromeOptions()\n",
    "\n",
    "# I recommend to use the headless option at least, out of the 3\n",
    "option.add_argument('--headless')\n",
    "option.add_argument('--no-sandbox')\n",
    "option.add_argument('--disable-dev-sh-usage')\n",
    "\n",
    "# Replace YOUR-PATH-TO-CHROMEDRIVER with your chromedriver location, locate the chromedriver.exe, right click, choose copy as path\n",
    "# check your chrome version at chrome browser setting, about chrome, then download chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\wibow\\OneDrive\\Desktop\\Web Scraping\\chromedriver_win32\\chromedriver.exe', options=option)\n",
    "\n",
    "driver.get('https://www.imdb.com/chart/top/') # Getting page HTML through request\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser') # Parsing content using beautifulsoup. Notice driver.page_source instead of page.content\n",
    " \n",
    "links = soup.select(\"table tbody tr td.titleColumn a\") # Selecting all of the anchors with titles\n",
    "first10 = links[:10] # Keep only the first 10 anchors\n",
    "for anchor in first10:\n",
    "    print(anchor.text) # Display the innerText of each anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088a915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to Extract Statically Loaded Content Using Selenium\n",
    "Using the code from above, we can now access each movie page by calling the click method on each of the \n",
    "anchors.\n",
    "'''\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "first_link = driver.find_elements(By.CSS_SELECTOR,'table tbody tr td.titleColumn a')[0]\n",
    "first_link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaf77d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'The Shawshank Redemption', 'year': '1994', 'duration': '2h 22m'}, {'title': 'The Godfather', 'year': '1972', 'duration': '2h 55m'}, {'title': 'The Dark Knight', 'year': '2008', 'duration': '2h 32m'}, {'title': 'The Godfather Part II', 'year': '1974', 'duration': '3h 22m'}, {'title': '12 Angry Men', 'year': '1957', 'duration': '1h 36m'}, {'title': \"Schindler's List\", 'year': '1993', 'duration': '3h 15m'}, {'title': 'The Lord of the Rings: The Return of the King', 'year': '2003', 'duration': '3h 21m'}, {'title': 'Pulp Fiction', 'year': '1994', 'duration': '2h 34m'}, {'title': 'The Lord of the Rings: The Fellowship of the Ring', 'year': '2001', 'duration': '2h 58m'}, {'title': 'The Good, the Bad and the Ugly', 'year': '1966', 'duration': '2h 58m'}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This will simulate a click on the first movie’s link. However, in this case, I recommend that you continue \n",
    "using driver.get instead. This is because you will no longer be able to use the click() method after you go \n",
    "on a different page since the new page doesn't have links to the other nine movies.\n",
    "\n",
    "As a result, after clicking on the first title from the list, you’d need to go back to the first page, then \n",
    "click on the second, and so on. This is a waste of performance and time. Instead, we will just use the \n",
    "extracted links and access them one by one.\n",
    "\n",
    "For “The Shawshank Redemption”, the movie page will be https://www.imdb.com/title/tt0111161/. We will extract \n",
    "the movie’s year and duration from the page, but this time we will use Selenium’s functions instead of \n",
    "BeautifulSoup as an example. In practice, you can use either one, so pick your favorite.\n",
    "\n",
    "To retrieve the movie’s year and duration, you should repeat the first step we went through here on the \n",
    "movie’s page.\n",
    "\n",
    "You will notice that you can find all of the information in the first element with the class ipc-inline-list \n",
    "(\".ipc-inline-list\" selector) and that all of the elements of the list contain the attribute role with the \n",
    "value presentation (the [role=’presentation’] selector).\n",
    "'''\n",
    "\n",
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# define option\n",
    "option = webdriver.ChromeOptions()\n",
    "\n",
    "# put option below, for this time no need for the option\n",
    "\n",
    "# Replace YOUR-PATH-TO-CHROMEDRIVER with your chromedriver location\n",
    "service = Service(r'C:\\Users\\wibow\\OneDrive\\Desktop\\Web Scraping\\chromedriver_win32\\chromedriver.exe')\n",
    "\n",
    "# pass service object\n",
    "driver = webdriver.Chrome(service=service, options=option)\n",
    " \n",
    "page = driver.get('https://www.imdb.com/chart/top/') # Getting page HTML through request\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser') # Parsing content using beautifulsoup\n",
    " \n",
    "totalScrapedInfo = [] # In this list we will save all the information we scrape\n",
    "links = soup.select(\"table tbody tr td.titleColumn a\") # Selecting all of the anchors with titles\n",
    "first10 = links[:10] # Keep only the first 10 anchors\n",
    "for anchor in first10:\n",
    "    driver.get('https://www.imdb.com/' + anchor['href']) # Access the movie’s page\n",
    "    infolist = driver.find_elements(By.CSS_SELECTOR, '.ipc-inline-list')[1] # Find the element with class ‘ipc-inline-list’ consisting year, rating, duration, in this page its the second element block, check by inspect one of the title page\n",
    "    informations = infolist.find_elements(By.CSS_SELECTOR, \"[role='presentation']\") # Find all elements with role=’presentation’ from the second element with class ‘ipc-inline-list’\n",
    "    scrapedInfo = {\n",
    "        \"title\": anchor.text, # fill the title with anchor\n",
    "        \"year\": informations[0].text, # year is the first element with presentation role \n",
    "        \"duration\": informations[2].text, # duration is the second\n",
    "    } # Save all the scraped information in a dictionary\n",
    "    totalScrapedInfo.append(scrapedInfo) # Append the dictionary to the totalScrapedInformation list\n",
    "\n",
    "print(totalScrapedInfo) # Display the list with all the information we scraped\n",
    "\n",
    "driver.quit() # Close the WebDriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065eeda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'The Shawshank Redemption', 'year': '1994', 'duration': '2h 22m', 'editorial-list': [\"What's New on HBO and HBO Max in January 2022\", \"What's New on Netflix in March 2022\", 'Everything Coming to HBO and HBO Max in August 2021']}, {'title': 'The Godfather', 'year': '1972', 'duration': '2h 55m', 'editorial-list': ['10 films that inspired director George Tillman Jr.', 'All Oscar Best Picture winners, ranked by IMDb rating', 'New on Netflix India This Aug 2020']}, {'title': 'The Dark Knight', 'year': '2008', 'duration': '2h 32m', 'editorial-list': ['The Billion-Dollar Film Club: 50+ Movies to Reach $1 Billion Worldwide', 'The billion-dollar superhero club: All the superhero movies to reach $1 billion worldwide', \"What's new on Hulu in December 2022\"]}, {'title': 'The Godfather Part II', 'year': '1974', 'duration': '3h 22m', 'editorial-list': ['All Oscar Best Picture winners, ranked by IMDb rating', 'Top 100 Movies Bucket List', 'Top 100 Movies as Rated by Women on IMDb in 2016']}, {'title': '12 Angry Men', 'year': '1957', 'duration': '1h 36m', 'editorial-list': [\"What's new on Prime Video in March 2023\", 'Top 100 Movies Bucket List']}, {'title': \"Schindler's List\", 'year': '1993', 'duration': '3h 15m', 'editorial-list': ['All Oscar Best Picture winners, ranked by IMDb rating', 'New on Netflix India This April 2021', 'New on Netflix India This July 2020']}, {'title': 'The Lord of the Rings: The Return of the King', 'year': '2003', 'duration': '3h 21m', 'editorial-list': ['The Billion-Dollar Film Club: 50+ Movies to Reach $1 Billion Worldwide', 'All Oscar Best Picture winners, ranked by IMDb rating', \"What's new on Netflix in February 2023\"]}, {'title': 'Pulp Fiction', 'year': '1994', 'duration': '2h 34m', 'editorial-list': [\"What's new on Prime Video in March 2023\", 'Movies That Changed My Life', 'Everything Coming to HBO Max in January 2021']}, {'title': 'The Lord of the Rings: The Fellowship of the Ring', 'year': '2001', 'duration': '2h 58m', 'editorial-list': [\"What's new on HBO and HBO Max in May 2023\", 'This beautiful planet: Films and series that celebrate Earth', \"What's new on Netflix in February 2023\"]}, {'title': 'The Good, the Bad and the Ugly', 'year': '1966', 'duration': '2h 58m', 'editorial-list': [\"What's New on Prime Video in February 2022\", 'Everything Coming to HBO and HBO Max in September 2021', 'Everything Coming to Netflix in April 2020']}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "How to Extract Dynamically Loaded Content Using Selenium\n",
    "The next big step in web scraping is extracting content that is loaded dynamically. You can find such content on \n",
    "each of the movie’s pages (such as https://www.imdb.com/title/tt0111161/) in the Editorial Lists section.\n",
    "\n",
    "If you look using inspect on the page, you'll see that you can find the section as an element with the attribute \n",
    "data-testid set as firstListCardGroup-editorial. But if you look in the page source, you will not find this \n",
    "attribute value anywhere. That’s because the Editorial Lists section is loaded by IMDB dynamically.\n",
    "\n",
    "In the following example, we will scrape the editorial list of each movie and add it to our current results of \n",
    "the total scraped information.\n",
    "\n",
    "To do that, we will import a few more packages that make it possible to wait for our dynamic content to load.\n",
    "'''\n",
    "\n",
    "# import library\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    " \n",
    " # define option\n",
    "option = webdriver.ChromeOptions()\n",
    "\n",
    "# put option below, for this time no need for the option\n",
    "\n",
    "# Replace YOUR-PATH-TO-CHROMEDRIVER with your chromedriver location\n",
    "service = Service(r'C:\\Users\\wibow\\OneDrive\\Desktop\\Web Scraping\\chromedriver_win32\\chromedriver.exe')\n",
    "\n",
    "# pass service object\n",
    "driver = webdriver.Chrome(service=service, options=option)\n",
    " \n",
    "page = driver.get('https://www.imdb.com/chart/top/') # Getting page HTML through request\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser') # Parsing content using beautifulsoup\n",
    " \n",
    "totalScrapedInfo = [] # In this list we will save all the information we scrape\n",
    "links = soup.select(\"table tbody tr td.titleColumn a\") # Selecting all of the anchors with titles\n",
    "first10 = links[:10] # Keep only the first 10 anchors\n",
    "for anchor in first10:\n",
    "    driver.get('https://www.imdb.com/' + anchor['href']) # Access the movie’s page\n",
    "    infolist = driver.find_elements(By.CSS_SELECTOR, '.ipc-inline-list')[1] # Find the element with class ‘ipc-inline-list’ consisting year, rating, duration, in this page its the second element block, check by inspect one of the title page\n",
    "    informations = infolist.find_elements(By.CSS_SELECTOR, \"[role='presentation']\") # Find all elements with role=’presentation’ from the second element with class ‘ipc-inline-list’\n",
    "    scrapedInfo = {\n",
    "        \"title\": anchor.text, # fill the title with anchor\n",
    "        \"year\": informations[0].text, # year is the first element with presentation role \n",
    "        \"duration\": informations[2].text, # duration is the second\n",
    "    } # Save all the scraped information in a dictionary\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll automatically to the bottom of the page to load all the page content otherwise : error timeout because element not appear\n",
    "    WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-testid='firstListCardGroup-editorial']\"))) # We are waiting for 60 seconds for our element with the attribute data-testid set as `firstListCardGroup-editorial`\n",
    "    listElements = driver.find_elements(By.CSS_SELECTOR,\"[data-testid='firstListCardGroup-editorial'] .sc-77f04bb0-1.hEkxEA.listName\") # Extracting the editorial lists elements\n",
    "    listNames = [] # Creating an empty list and then appending only the elements texts\n",
    "    for el in listElements:\n",
    "        listNames.append(el.text)\n",
    "    scrapedInfo['editorial-list'] = listNames # Adding the editorial list names to our scrapedInfo dictionary\n",
    "    totalScrapedInfo.append(scrapedInfo) # Append the dictionary to the totalScrapedInformation list\n",
    "    \n",
    "print(totalScrapedInfo) # Display the list with all the information we scraped\n",
    "\n",
    "driver.quit() # Close the WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2091bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to Save the Scraped Content\n",
    "Now that we have all the data we want, we can save it as a .json or a .csv file for easier readability.\n",
    "\n",
    "To do that, we will just use the JSON and CVS packages from Python and write our content to new files:\n",
    "'''\n",
    "\n",
    "'''\n",
    "# uncomment code below to run\n",
    "import csv\n",
    "import json\n",
    "        \n",
    "# Save scraped data to JSON file\n",
    "with open('scraped_data.json', 'w') as json_file: # the json file will be named scraped_data\n",
    "    json.dump(totalScrapedInfo, json_file)\n",
    "\n",
    "# Save scraped data to CSV file\n",
    "csv_file_path = 'scraped_data.csv'\n",
    "fieldnames = totalScrapedInfo[0].keys()\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(totalScrapedInfo)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
